---
title: "Breaking an LSTM Model"
output: html_notebook
---

```{r}
library(readr)
library(dplyr)
library(keras)
library(tidyr)
```



```{r}
oil <- read_csv('C:\\Users\\Christopher\\Desktop\\Data\\oil_prices.csv')
```
```{r}
head(oil)
```

```{r}
oil <- oil %>%
  rename(price = DCOILBRENTEU) %>%
  mutate(price = as.numeric(price)) %>%
  mutate(change64_days = lag(price, 64) ) %>%
  mutate (id = (1 - 64):( nrow(oil) - 64) ) %>%
  drop_na() %>%
  mutate (change64_days = price - change64_days )
summary(oil)
```

```{r}
timesteps = 32
batch_size = 64
epochs = 120
```

```{r}
past <- NULL
future <- NULL
temp <- oil[32:NROW(oil), 'price']
# indexing to avoid NAs in past data; could do same for future, but that would eliminate data for real predictions
for (i in 1:timesteps){
  past <- c(past,unlist(lag( oil$price, i)))
  future <- c(future,unlist(lead( oil$price, i-1)))
}
past <-   array( data =   past, dim = c(NROW(oil), 32, 1) )
future <- array( data = future, dim = c(NROW(oil), 32, 1) )

#past <- aperm(past, c(2,1,3) )
#future <- aperm(future, c(2,1,3) )

clip = timesteps + 1 + ((dim(future)[1] - timesteps) %% batch_size)

#future <- future[,clip:dim(future)[2],1]
#past <- past[,clip:dim(past)[2],1]
future <- future[clip:dim(future)[1],,1]
past   <- past  [clip:dim(  past)[1],,1]
```

```{r, scaling}
# prevent accidental rerunning when playing with code
if ( max(past) >1 || min(past) < 0 ){
  future <- ( future - min ( oil$price ) ) / ( max( oil$price - min ( oil$price ) ) )
  past <- ( past - min ( oil$price ) ) / (max( oil$price - min( oil$price ) ) )
}
future <- array(future,c(dim(future)[1], dim(future)[2], 1))
past <- array(past,c(dim(past)[1], dim(past)[2], 1))
```


```{r}
train_share <- 0.90
batches <- dim(future)[1] / batch_size
train_batches <- as.integer((batches - 1) * 0.9)
test_batches <- batches - 1 - train_batches
train_end <- train_batches * batch_size
test_start <- train_end + 1
test_end <- train_end + test_batches * batch_size
future_start <- test_end +1
future_end <- dim(future)[2]
```

```{r}
model <- keras_model_sequential() %>%
  layer_lstm(10, batch_input_shape = c(64,32,1),stateful = TRUE, return_sequences = TRUE) %>%
  layer_dropout(0.2) %>%
  layer_lstm(10, batch_input_shape = c(64,32,1),stateful = TRUE, return_sequences = TRUE) %>%
  layer_dropout(0.2) %>%
  layer_dense(1)
summary(model)
```

```{r}
train_X <- past[1:train_end,,]
train_X <- array(train_X,c(dim(train_X)[1], dim(train_X)[2], 1))

train_y <- future[1:train_end,,]
train_y <- array(train_y,c(dim(train_y)[1], dim(train_y)[2], 1))
model %>% 
  compile(
    loss = 'mse',
    optimizer = 'adam'
) %>%
  fit(
    epochs = epochs,
    batch_size = batch_size,
    x = train_X,
    y = train_y,
    validation_split = (11 / 111)
#    validation_data = c(test_X, test_y)
  )
```
```{r}
simple_model <- keras_model_sequential() %>%
  layer_lstm(10, batch_input_shape = c(64,32,1),stateful = TRUE, return_sequences = TRUE) %>%
  layer_lstm(10, batch_input_shape = c(64,32,1),stateful = TRUE, return_sequences = TRUE) %>%
  layer_dense(1)
summary(simple_model)
```

```{r}
simple_model %>% 
  compile(
    loss = 'mse',
    optimizer = 'adam'
) %>%
  fit(
    epochs = epochs,
    batch_size = batch_size,
    x = train_X,
    y = train_y
  )
```

```{r}
test_X <- past[test_start:test_end,,]
test_X <- array(test_X,c(dim(test_X)[1], dim(test_X)[2], 1))

test_y <- future[test_start:test_end,,]
test_y <- array(test_y,c(dim(test_y)[1], dim(test_y)[2], 1))

pred <- predict(model, past, batch_size = 64)
model %>%
  evaluate(batch_size = 64,test_X, test_y, verbose = 0)
```
```{r}
simple_pred <- predict(simple_model, past, batch_size = 64)
model %>%
  evaluate(batch_size = 64, test_X, test_y, verbose = 0)
```


```{r}
R2 <- function(y, yhat){
  return((1 - sum( ( y - yhat )^2 ) / sum( ( y - mean(y) )^2 ) ) * 100)
}
```

```{r}
print(paste('R-Squared for all test data:', R2(future[test_start:test_end,,], pred[test_start:test_end,,] )) )
for (i in 1:32){
  print(paste('At',i,'R-Squared:',R2(future[test_start:test_end,i,], pred[test_start:test_end,i,])))
}
```

```{r}
print(paste('R-Squared for all test data:', R2(future[test_start:test_end,,], simple_pred[test_start:test_end,,] )) )
for (i in 1:32){
  print(paste('At',i,'R-Squared:',R2(future[test_start:test_end,i,], simple_pred[test_start:test_end,i,])))
}
```



```{r}
tensorplot <- function(y, yhat, xStart = 1, xEnd = NA,x = NA, ySlice = NA, actual = 1){
  timesteps <- dim(y)[2]
  if (all(is.na(xEnd) ) ){
    len <- dim(y)[1]
    xEnd <- dim(y)[1] + dim(y)[2] -1
  }
  else{
    len <- xEnd
    xEnd <- xEnd + dim(y)[2] -1
  }
  if (all(is.na(x) ) ){
    x = xStart:xEnd
    print(length(x))
  }
  if (all(is.na(ySlice) ) ){
    ySlice = 1:dim(y)[2]
  }
  plot(range(x), range(yhat[,ySlice,1] ), type = 'n')
  increment <- 360 / length(ySlice)
  hue <- 0
  for (i in ySlice){
    ends <- timesteps - i
    col <- hcl(h = hue, c = 50, l = 85, alpha = 0.15)
    print(length((xStart + ends):(len + ends)))
    print(length(unlist(yhat[xStart:len,i,1])))
    points((xStart + i):(len + i), unlist(yhat[xStart:len,i,1]), col = col, pch = 19, cex = 1.2)
    hue = hue + increment
  }
  i <- 1
  ends <- timesteps - i
  col <- 'black'
  lines((xStart + i):(len + i), unlist(y[xStart:len,i,1]), col = col)
}
```


```{r}
tensorplot(future, pred, xStart = test_start, ySlice = c(2) )
#tensorplot(future, future, xStart = test_start, xEnd = 7000, ySlice = c(15) )
```
```{r}
tensorplot(future, simple_pred, xStart = test_start, ySlice = c(2) )
```
## Performance

Model performs better without validation data
