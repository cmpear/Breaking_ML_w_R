---
title: "Breaking an LSTM Model"
output: html_notebook
---

# INTRODUCTION

The goal of this project is to explore stateful LSTM models.  The two underlying questions it seeks to answer are whether incuding validation data in a stateful model helps in the same manner as it does for non-stateful models, and whether the placement of the division between training and testing data can potentially 'break' the model so that the R-Squared value goes negative.

## Suspicions

### Validation Data and Stateful Models

Unlike for non-stateful models, tutorials for stateful models rarely include the use of validation data.  One possible reason for this is that stateful models, unlike normal models, doe not divide their data into groups randomly.  Data is fed into the model in order from earliest to latest (or sometimes the reverse).  Thus, the training data is the most in the past, and the testing data is the most recent.  Any validation data would have to consist of blocks of data in between those two, and would not be random.  Thus, validation data would present two dangers: overfitting on the validation data itself, and a gap between training and testing data.

### Breaking a Model

When running some tests with stock data, there were a few cases where the model thoroughly failed and the predictions yielded negative R-Squared values.  One common feature of those cases was that the division between training and testing data coincided with a sudden spike in the stock price, which was followed by a period with a new average.  Do these conditions consistently break the model?  And can we clarify the conditions for breaking the model?


## Oil Price Data

For testing these hypotheses, we are going to make use of a dataset of oil prices since 1989.  The dataset was chosen because it is not only data of interest, but also because the data is rich, and the price of oil experienced several sudden climbs followed by long plateaus.  Thus, if we are trying to choose a dividing line between training and test data that could cause the model to fail, there should be several good options with enough data to train and test the model. 

```{r}
library(readr)
library(dplyr)
library(keras)
library(tidyr)
```

```{r}
# not built for laptop yet
oil <- read_csv('C:\\Users\\Christopher\\Desktop\\Data\\oil_prices.csv')
```
```{r}
head(oil)
```

```{r}
oil <- oil %>%
  rename(price = DCOILBRENTEU) %>%
  mutate(price = as.numeric(price)) %>%
  mutate(change64_days = lag(price, 64) ) %>%
  mutate (id = (1 - 64):( nrow(oil) - 64) ) %>%
  drop_na() %>%
  mutate (change64_days = price - change64_days )
summary(oil)
```

```{r}
timesteps = 32
batch_size = 64
epochs = 120
```

```{r}
past <- NULL
future <- NULL
temp <- oil[32:NROW(oil), 'price']
# indexing to avoid NAs in past data; could do same for future, but that would eliminate data for real predictions
for (i in 1:timesteps){
  past <- c(past,unlist(lag( oil$price, i)))
  future <- c(future,unlist(lead( oil$price, i-1)))
}
past <-   array( data =   past, dim = c(NROW(oil), 32, 1) )
future <- array( data = future, dim = c(NROW(oil), 32, 1) )

clip = timesteps + 1 + ((dim(future)[1] - timesteps) %% batch_size)

future <- future[clip:dim(future)[1],,1]
past   <- past  [clip:dim(  past)[1],,1]
```

```{r, scaling}
# prevent accidental rerunning when playing with code
if ( max(past) >1 || min(past) < 0 ){
  future <- ( future - min ( oil$price ) ) / ( max( oil$price - min ( oil$price ) ) )
  past <- ( past - min ( oil$price ) ) / (max( oil$price - min( oil$price ) ) )
}
future <- array(future,c(dim(future)[1], dim(future)[2], 1))
past <- array(past,c(dim(past)[1], dim(past)[2], 1))
```


```{r}
train_share <- 0.90
batches <- dim(future)[1] / batch_size
train_batches <- as.integer((batches - 1) * 0.9)
test_batches <- batches - 1 - train_batches
train_end <- train_batches * batch_size
test_start <- train_end + 1
test_end <- train_end + test_batches * batch_size
future_start <- test_end +1
future_end <- dim(future)[2]
```

```{r}
model <- keras_model_sequential() %>%
  layer_lstm(10, batch_input_shape = c(64,32,1),stateful = TRUE, return_sequences = TRUE) %>%
  layer_dropout(0.2) %>%
  layer_lstm(10, batch_input_shape = c(64,32,1),stateful = TRUE, return_sequences = TRUE) %>%
  layer_dropout(0.2) %>%
  layer_dense(1)
summary(model)
```

```{r}
train_X <- past[1:train_end,,]
train_X <- array(train_X,c(dim(train_X)[1], dim(train_X)[2], 1))

train_y <- future[1:train_end,,]
train_y <- array(train_y,c(dim(train_y)[1], dim(train_y)[2], 1))
model %>% 
  compile(
    loss = 'mse',
    optimizer = 'adam'
) %>%
  fit(
    epochs = epochs,
    batch_size = batch_size,
    x = train_X,
    y = train_y,
    validation_split = (11 / 111)
#    validation_data = c(test_X, test_y)
  )
```
```{r}
simple_model <- keras_model_sequential() %>%
  layer_lstm(10, batch_input_shape = c(64,32,1),stateful = TRUE, return_sequences = TRUE) %>%
  layer_lstm(10, batch_input_shape = c(64,32,1),stateful = TRUE, return_sequences = TRUE) %>%
  layer_dense(1)
summary(simple_model)
```

```{r}
simple_model %>% 
  compile(
    loss = 'mse',
    optimizer = 'adam'
) %>%
  fit(
    epochs = epochs,
    batch_size = batch_size,
    x = train_X,
    y = train_y
  )
```

```{r}
test_X <- past[test_start:test_end,,]
test_X <- array(test_X,c(dim(test_X)[1], dim(test_X)[2], 1))

test_y <- future[test_start:test_end,,]
test_y <- array(test_y,c(dim(test_y)[1], dim(test_y)[2], 1))

pred <- predict(model, past, batch_size = 64)
model %>%
  evaluate(batch_size = 64,test_X, test_y, verbose = 0)
```
```{r}
simple_pred <- predict(simple_model, past, batch_size = 64)
model %>%
  evaluate(batch_size = 64, test_X, test_y, verbose = 0)
```


```{r}
R2 <- function(y, yhat){
  return((1 - sum( ( y - yhat )^2 ) / sum( ( y - mean(y) )^2 ) ) * 100)
}
```

```{r}
print(paste('R-Squared for all test data:', R2(future[test_start:test_end,,], pred[test_start:test_end,,] )) )
for (i in 1:32){
  print(paste('At',i,'R-Squared:',R2(future[test_start:test_end,i,], pred[test_start:test_end,i,])))
}
```

```{r}
print(paste('R-Squared for all test data:', R2(future[test_start:test_end,,], simple_pred[test_start:test_end,,] )) )
for (i in 1:32){
  print(paste('At',i,'R-Squared:',R2(future[test_start:test_end,i,], simple_pred[test_start:test_end,i,])))
}
```



```{r}
tensorplot <- function(y, yhat, xStart = 1, xEnd = NA,x = NA, ySlice = NA, actual = 1){
  timesteps <- dim(y)[2]
  if (all(is.na(xEnd) ) ){
    len <- dim(y)[1]
    xEnd <- dim(y)[1] + dim(y)[2] -1
  }
  else{
    len <- xEnd
    xEnd <- xEnd + dim(y)[2] -1
  }
  if (all(is.na(x) ) ){
    x = xStart:xEnd
    print(length(x))
  }
  if (all(is.na(ySlice) ) ){
    ySlice = 1:dim(y)[2]
  }
  plot(range(x), range(yhat[,ySlice,1] ), type = 'n')
  increment <- 360 / length(ySlice)
  hue <- 0
  for (i in ySlice){
    ends <- timesteps - i
    col <- hcl(h = hue, c = 50, l = 85, alpha = 0.15)
    print(length((xStart + ends):(len + ends)))
    print(length(unlist(yhat[xStart:len,i,1])))
    points((xStart + i):(len + i), unlist(yhat[xStart:len,i,1]), col = col, pch = 19, cex = 1.2)
    hue = hue + increment
  }
  i <- 1
  ends <- timesteps - i
  col <- 'black'
  lines((xStart + i):(len + i), unlist(y[xStart:len,i,1]), col = col)
}
```


```{r}
tensorplot(future, pred, xStart = test_start, ySlice = c(2) )
#tensorplot(future, future, xStart = test_start, xEnd = 7000, ySlice = c(15) )
```
```{r}
tensorplot(future, simple_pred, xStart = test_start, ySlice = c(2) )
```



```{r}
y_sim <- seq(from = 1, to = 6000) / 1500 +
  c(rep(1, 200), rep(2, 1800), rep(1, 200),rep(3, 800),rep(2, 200), rep(5, 1000), rep (10, 1800)) +
  rnorm(n = 6000, mean = 0, sd = 1)
y_sim <- (y_sim - min(y_sim) ) / (max(y_sim) - min(y_sim) )
```

```{r}
max(y_sim)
```




## Performance

Model performs better without validation data
